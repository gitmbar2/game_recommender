{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model\n",
    "\n",
    "### from playtime_distribution, we found 2-5, 6-25, 26+ as good initial cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# needs python 2\n",
    "# import graphlab as gl\n",
    "# gl.canvas.set_target('ipynb')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from src import EDA\n",
    "from src import ModelEvaluation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Maybe show playtime distribution picture here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>game_name</th>\n",
       "      <th>purchase_action</th>\n",
       "      <th>playtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151603712</td>\n",
       "      <td>The Elder Scrolls V Skyrim</td>\n",
       "      <td>play</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151603712</td>\n",
       "      <td>Fallout 4</td>\n",
       "      <td>play</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid                   game_name purchase_action  playtime\n",
       "1  151603712  The Elder Scrolls V Skyrim            play     273.0\n",
       "3  151603712                   Fallout 4            play      87.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam_df = EDA.load_200k_n_games_played(5)\n",
    "steam_df = steam_df[steam_df['purchase_action'] == 'play']\n",
    "steam_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some normalization needs to be done for played time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_df[\"playtime_rank\"] = steam_df['playtime'].map(lambda value: EDA.rank_playtime(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game names need to be changed to IDs for Spark ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting ALS must have numbers for itemCol and userCol\n",
    "game_uid = 0\n",
    "game_uid_map = {}\n",
    "for item in steam_df['game_name']:\n",
    "    if item in game_uid_map:\n",
    "        continue\n",
    "    game_uid_map[item] = game_uid\n",
    "    game_uid += 1\n",
    "steam_df['game_uid'] = steam_df['game_name'].map(lambda name: game_uid_map[name])\n",
    "steam_df['game_uid'].value_counts().size == steam_df['game_name'].value_counts().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-User vs Item-Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  2436\n",
      "Number of games:  3544\n"
     ]
    }
   ],
   "source": [
    "# Spark ML seems to default to user-user\n",
    "# TODO check documentation for user-user vs item-item\n",
    "print('Number of users: ', steam_df['uid'].value_counts().size)\n",
    "print('Number of games: ', steam_df['game_name'].value_counts().size)\n",
    "# Ideally we would use the games since there are less games than users (faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ALS Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  46128\n",
      "Test size:  11661\n"
     ]
    }
   ],
   "source": [
    "# Setup a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Convert a Pandas DF to a Spark DF\n",
    "spark_df = spark.createDataFrame(steam_df)\n",
    "spark_df.count()\n",
    "train, test = spark_df.randomSplit([0.8, 0.2], seed=427471138)\n",
    "print('Training size: ', train.count())\n",
    "print('Test size: ', test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als_model = ALS(\n",
    "    itemCol='game_uid',\n",
    "    userCol='uid',\n",
    "    ratingCol='playtime_rank',\n",
    "    nonnegative=True,    \n",
    "    regParam=0.1,\n",
    "    coldStartStrategy=\"drop\", # Drops if user or item in test was not in train\n",
    "    rank=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitted_als_model = als_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+\n",
      "|game_uid|      uid|prediction|\n",
      "+--------+---------+----------+\n",
      "|       1|151603712| 1.3429985|\n",
      "+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_row_pandas_df = pd.DataFrame({'uid': [151603712], 'game_uid': [1]})\n",
    "one_row_spark_df = spark.createDataFrame(one_row_pandas_df)\n",
    "fitted_als_model.transform(one_row_spark_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = fitted_als_model.transform(test)\n",
    "evaluator = RegressionEvaluator() \\\n",
    "    .setMetricName(\"rmse\") \\\n",
    "    .setLabelCol(\"playtime_rank\") \\\n",
    "    .setPredictionCol(\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0051786784416223"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse\n",
    "# was 1.046 without restricting to 5+\n",
    "# was 1.005 with 5+, without normalizing hours played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uid=116876958, game_name='No Time To Explain Remastered', purchase_action='play', playtime=3.4, playtime_rank=1, game_uid=148, prediction=0.8219438791275024),\n",
       " Row(uid=11373749, game_name='Shadow Puppeteer', purchase_action='play', playtime=0.6, playtime_rank=0, game_uid=463, prediction=0.0),\n",
       " Row(uid=45617627, game_name='Tomb Raider I', purchase_action='play', playtime=0.2, playtime_rank=0, game_uid=471, prediction=0.8916951417922974),\n",
       " Row(uid=101687527, game_name='Tomb Raider I', purchase_action='play', playtime=0.7, playtime_rank=0, game_uid=471, prediction=0.9380249977111816),\n",
       " Row(uid=44472980, game_name='Mortal Kombat Komplete Edition', purchase_action='play', playtime=505.0, playtime_rank=3, game_uid=496, prediction=1.6772217750549316)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO find examples of each item rank to get a sense of types of items\n",
    "# TODO better evaluation than rmse - this is not good if recommending a small subset of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to recommend on test set\n",
    "userRecs = fitted_als_model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(uid=46014950, recommendations=[Row(game_uid=832, rating=2.1705732345581055), Row(game_uid=2375, rating=2.0654635429382324), Row(game_uid=2034, rating=2.061188220977783), Row(game_uid=2857, rating=2.0255558490753174), Row(game_uid=214, rating=1.932112216949463)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6309297535714578"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelEvaluation.dcg_at_k([1,2,1],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1150dc208>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.groupBy('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
